# 分布式理论及架构设计 



# 第1章 分布式理论

(授课老师: 墨竹)

![img](Design.assets/wps1.png)

## 1.1 分布式架构系统回顾

#### 1.1.1 分布式系统概念

> 分布式系统是一个硬件或软件组件分布在不同的网络计算机上，彼此之间仅仅通过消息传递进行通信和协调 的系统。

俗的理解，所谓分布式系统，就是一个业务拆分成多个子业务，分布在不同的服务器节点，共同构成的系统称为分  布式系统，同一个分布式系统中的服务器节点在空间部署上是可以随意分布的，这些服务器可能放在不同的机柜   中，也可能在不同的机房中，甚至分布在不同的城市。

![img](Design.assets/wps4-1610369646645.png)


Tip：

分布式与集群的区别：

```
集群：多个人在一起作同样的事 。
分布式 ：多个人在一起作不同的事 。
```

![img](Design.assets/wps6-1610369650083.png) 

分布式系统的特点：

（1） 分布性

（2） 对等性

（3） 并发性

（4） 缺乏全局时钟

（5） 故障总是会发生

#### 1.1.2 分布式系统的发展

阿里巴巴发起的"去 IOE"运动 (IOE 指的是 IBM 小型机、Oracle 数据库、EMC 的高端存储)。阿里巴巴2009 年“去

IOE”战略技术总监透露，截止到 2013 年 5 月 17 日阿里巴巴最后一台 IBM 小型机在支付宝下线。

为什么要去IOE

1. 升级单机处理能力的性价比越来越低

2. 单机处理能力存在瓶颈

3. 稳定性和可用性这两个指标很难达到

#### 1.1.3 分布式架构的演变



![img](Design.assets/wps7.jpg) 


![img](Design.assets/wps8.jpg)


![img](Design.assets/wps9.jpg) 


![img](Design.assets/wps10.jpg) 



![img](Design.assets/wps11.jpg)



![img](Design.assets/wps12.jpg) 



![img](Design.assets/wps13.jpg) 



![img](Design.assets/wps14.jpg) 



![img](Design.assets/wps15.jpg) 



![img](Design.assets/wps16.jpg)






## 1.2 分布式系统面临的问题



1） 通信异常

网络本身的不可靠性，因此每次网络通信都会伴随着网络不可用的风险（光纤、路由、DNS等硬件设备或系统的不  可用），都会导致最终分布式系统无法顺利进行一次网络通信，另外，即使分布式系统各节点之间的网络通信能够  正常执行，其延时也会大于单机操作，存在巨大的延时差别，也会影响消息的收发过程，因此消息丢失和消息延迟  变的非常普遍。

2） 网络分区

网络之间出现了网络不连通，但各个子网络的内部网络是正常的，从而导致整个系统的网络环境被切分成了若干个  孤立的区域，分布式系统就会出现局部小集群，在极端情况下，这些小集群会独立完成原本需要整个分布式系统才  能完成的功能，包括数据的事务处理，这就对分布式一致性提出非常大的挑战。

3） 节点故障

节点故障是分布式系统下另一个比较常见的问题，指的是组成分布式系统的服务器节点出现的宕机或"僵死"现象，  根据经验来说，每个节点都有可能出现故障，并且经常发生.

4） 三态

分布式系统每一次请求与响应存在特有的“三态”概念，即成功、失败和超时。

分布式系统中，由于网络是不可靠的，虽然绝大部分情况下，网络通信能够接收到成功或失败的响应，但当网络出  现异常的情况下，就会出现超时现象，通常有以下两种情况：

1. 由于网络原因，该请求并没有被成功的发送到接收方，而是在发送过程就发生了丢失现象。

2. 该请求成功的被接收方接收后，并进行了处理，但在响应反馈给发送方过程中，发生了消息丢失现象。



## 1.3 分布式理论：一致性

#### 1.3.1 什么是分布式一致性

分布式数据一致性，指的是数据在多份副本中存储时，各副本中的数据是一致的。

#### 1.3.2 副本一致性

分布式系统当中，数据往往会有多个副本。如果是一台数据库处理所有的数据请求，那么通过ACID四原则，基本   可以保证数据的一致性。而多个副本就需要保证数据会有多份拷贝。这就带来了同步的问题，因为我们几乎没有办  法保证可以同时更新所有机器当中的包括备份所有数据。  网络延迟，即使我在同一时间给所有机器发送了更新数据的请求，也不能保证这些请求被响应的时间保持一致存在时间差，就会存在某些机器之间的数据不一致的情况。



![img](Design.assets/wps19.jpg) 

 

总得来说，我们无法找到一种能够满足分布式系统所有系统属性的分布式一致性解决方案。因此，如何既保证数据  的一致性，同时又不影响系统运行的性能，是每一个分布式系统都需要重点考虑和权衡的。于是，一致性级别由此  诞生：

#### 1.3.3 一致性分类

1、强一致性

这种一致性级别是最符合用户直觉的，它要求系统写入什么，读出来的也会是什么，用户体验好，但实现起来往往  对系统的性能影响大。但是强一致性很难实现。

2、弱一致性

这种一致性级别约束了系统在写入成功后，不承诺立即可以读到写入的值，也不承诺多久之后数据能够达到一致，  但会尽可能地保证到某个时间级别（比如秒级别）后，数据能够达到一致状态。

读写一致性


![img](Design.assets/wps20-1610369689517.png)


```
用户读取自己写入结果的一致性，保证用户永远能够第一时间看到自己更新的内容。
比如我们发一条朋友圈，朋友圈的内容是不是第一时间被朋友看见不重要，但是一定要显示在自己的列表上.

解决方案:
方案1：一种方案是对于一些特定的内容我们每次都去主库读取。 （问题主库压力大）
方案2：我们设置一个更新时间窗口，在刚刚更新的一段时间内，我们默认都从主库读取，过了这个窗口之后，我们会挑选最近有    过更新的从库进行读取
方案3：我们直接记录用户更新的时间戳，在请求的时候把这个时间戳带上，凡是最后更新时间小于这个时间戳的从库都不予以响应。
```


单调读一致性

```
本次读到的数据不能比上次读到的旧。
由于主从节点更新数据的时间不一致，导致用户在不停地刷新的时候，有时候能刷出来，再次刷新之后会发现数据不见了，再刷新又可能再刷出来，就好像遇见灵异事件一样

解决方案:就是根据用户ID计算一个hash值，再通过hash值映射到机器。同一个用户不管怎么刷新，都只会被映射到同一台机器上。这样就保证了不会读到其他从库的内容，带来用户体验不好的影响。
```


![img](Design.assets/wps23-1610369694969.png) 

 

因果一致性


```
指的是：如果节点 A 在更新完某个数据后通知了节点 B，那么节点 B 之后对该数据的访问和修改都是基于 A 更新后的值。于此同时，和节点 A 无因果关系的节点 C 的数据访问则没有这样的限制。
```

最终一致性


```
最终一致性是所有分布式一致性模型当中最弱的。可以认为是没有任何优化的“最”弱一致性，它的意思是说，我不考虑所有的中间    状态的影响，只保证当没有新的更新之后，经过一段时间之后，最终系统内所有副本的数据是正确的。
它最大程度上保证了系统的并发能力，也因此，在高并发的场景下，它也是使用最广的一致性模型。
```

![img](Design.assets/wps26-1610369698120.png)


## 1.4  分布式理论：CAP定理



CAP 定理

```
2000 年7月的时候，加州大学伯克利分校的Eric Brewer 教授提出了 CAP 猜想，2年后，被 来自于麻省理工的Seth Gilbert 和 Nancy Lynch 从理论上证明了猜想的可能性，从此，CAP 定理正式在学术上成为了分布式计算领域的公认定理。并深深的影响了分布式计算的发展。
```

CAP 理论含义是，一个分布式系统不可能同时满足一致性（C:Consistency)，可用性（A: Availability）和分区容错性（P：Partition tolerance）这三个基本需求，最多只能同时满足其中的2个。

 

| 选项 | 描述                                                         |
| -------------- | ------------------------------------------------------------ |
| C 一致性       | 分布式系统当中的一致性指的是所有节点的数据一致，或者说是所有副本的数据一致 |
| A 可用性       | Reads and writes always succeed. 也就是说系统一直可用，而且服务一直保持正常 |
| P 分区容错性   | 系统在遇到一些节点或者网络分区故障的时候，仍然能够提供满足一致性和可用性的服务 |



![img](Design.assets/wps30.jpg) 

 

C - Consistency

一致性是值写操作后读操作可以读到最新的数据状态,当数据分布在多个节点上时,从任意节点读取到的数据都是最  新的.

商品信息读写要满足一致性需要实现如下目标:

1. 商品服务写入主数据库成功, 则想从数据库查询数据也成功

2. 商品服务写入主数据库失败,则向从数据库查询也失败如何实现一致性?

1. 写入主数据库后要数据同步到从数据库

2. 写入主数据库后,在向从数据库同步期间要将从数据库锁定,  等待同步完成后在释放锁,以免在写新数据后,向从数据库查询到旧的数据.

分布式一致性的特点:

1. 由于存在数据库同步过程,写操作的响应会有一定的延迟

2. 为了保定数据的一致性,对资源暂时锁定,待数据同步完成后释放锁定资源

3. 如果请求数据同步失败的节点则会返回错误信息, 一定不会返回旧数据.

 

A - Availability

可用性是指任何操作都可以得到响应的结果,且不会出现响应超时或响应错误。商品信息读写要满足可用性需要实现如下目标:

1. 从数据库接收到数据库查询的请求则立即能够响应数据查询结果

2. 从数据库不允许出现响应超时或错误如何实现可用性?



1. 写入主数据库后要将数据同步到从数据

2. 由于要保证数据库的可用性,不可以将数据库中资源锁定

3. 即使数据还没有同步过来,从数据库也要返回查询数据, 哪怕是旧数据,但不能返回错误和超时.

 

P - Partition tolerance

分布式系统的各个节点部署在不同的子网中,  不可避免的会出现由于网络问题导致节点之间通信失败,此时仍可以对外提供服务, 这个就是分区容错性 (分区容忍性).

商品信息读写要满足分区容错性需要实现如下目标:

1. 主数据库想从数据库同步数据失败不形象写操作

2. 其中一个节点挂掉不会影响另一个节点对外提供服务如何实现分区容错性?

1. 尽量使用异步取代同步操作,举例 使用异步方式将数据从主数据库同步到从数据库,  这样节点之间能有效的实现松耦合;

2. 添加数据库节点,其中一个从节点挂掉,由其他从节点提供服务



CAP只能 3 选 2


![img](Design.assets/wps31-1610369710841.png)



![img](Design.assets/wps32-1610369712819.png) 

 

 

关于CAP这三个特性我们就介绍完了，接下来我们试着证明一下为什么CAP不能同时满足。  假设有一个系统如下：


![img](Design.assets/wps33.jpg) 



```
有用户向N1发送了请求更改了数据，将数据库从V0更新成了V1。由于网络断开，所以N2数据库依然是V0，如果这个时候有一个请求发给了N2，但是N2并没有办法可以直接给出最新的结果V1，这个时候该怎么办呢？

这个时候无法两种方法，一种是将错就错，将错误的V0数据返回给用户。第二种是阻塞等待，等待网络通信恢复，N2中的数据更新之后再返回给用户。显然前者牺牲了一致性，后者牺牲了可用性。

这个例子虽然简单，但是说明的内容却很重要。在分布式系统当中，CAP三个特性我们是无法同时满足的，必然要舍弃一个。三者舍弃一个，显然排列组合一共有三种可能。
```

 

1. 舍弃A(可用性)，保留CP(一致性和分区容错性)


```
一个系统保证了一致性和分区容错性，舍弃可用性。也就是说在极端情况下，允许出现系统无法访问的情况出现，这个时候往往会牺牲用户体验，让用户保持等待，一直到系统数据一致了之后，再恢复服务。
```




2. 舍弃C(一致性)，保留AP(可用性和分区容错性)

```
这种是大部分的分布式系统的设计，保证高可用和分区容错，但是会牺牲一致性。
```


3. 舍弃P(分区容错性)，保留CA(一致性和可用性)

```
如果要舍弃P，那么就是要舍弃分布式系统，CAP也就无从谈起了。可以说P是分布式系统的前提，所以这种情况是不存在的。
```


## 1.5 分布式理论：BASE 理论



什么是BASE理论

BASE：全称：Basically Available(基本可用)，Soh state（软状态）,和 Eventually consistent（最终一致性）三个短语的缩写，来自 ebay 的架构师提出。


BASE是对CAP中一致性和可用性权衡的结果，BASE理论的核心思想是：即使无法做到强一致性，但每个应用都可以   根据自身业务特点，采用适当的方式来使系统达到最终一致性。


①Basically Available(基本可用)

基本可用是指分布式系统在出现不可预知故障的时候，允许损失部分可用性——但请注意，这绝不等价于系统不可 用。以下就是两个"基本可用"的例子

- 响应时间上的损失：正常情况下，一个在线搜索引擎需要在0.5秒之内返回给用户相应的查询结果，但由于出  现故障（比如系统部分机房发生断电或断网故障），查询结果的响应时间增加到了1~2秒。

- 功能上的损失：正常情况下，在一个电子商务网站（比如淘宝）上购物，消费者几乎能够顺利地完成每一笔  订单。但在一些节日大促购物高峰的时候（比如双十一、双十二），由于消费者的购物行为激增，为了保护  系统的稳定性（或者保证一致性），部分消费者可能会被引导到一个降级页面，如下：



![img](Design.assets/wps41-1610369719119.png) 


②Soh state（软状态）

什么是软状态呢？相对于一致性，要求多个节点的数据副本都是一致的，这是一种 “硬状态”。

软状态指的是：允许系统中的数据存在中间状态，并认为该状态不影响系统的整体可用性，即允许系统在多个不同  节点的数据副本之间进行数据同步的过程中存在延迟。

 

③Eventually consistent（最终一致性）

最终一致性强调的是系统中所有的数据副本，在经过一段时间的同步后，最终能够达到一个一致的状态。因此最终  一致性的本质是需要系统保证最终数据能够达到一致，而不需要实时保证系统数据的强一致性。

 

## 1.6 分布式理论：分布式事务

#### 1.6.1 数据库事务回顾

事务的基本特性：

我们知道事务有4个非常重要的特性，即我们常说的（ACID）。

Atomicity（原子性）:是说事务是一个不可分割的整体，所有操作要么全做，要么全不做；只要事务中有一个操作   出错，回滚到事务开始前的状态的话，那么之前已经执行的所有操作都是无效的，都应该回滚到开始前的状态。



Consistency（一致性）：是说事务执行前后，数据从一个状态到另一个状态必须是一致的，比如A向B转账（A、B    的总金额就是一个一致性状态），不可能出现A扣了钱，B却没收到的情况发生。

Isolation（隔离性）：多个并发事务之间相互隔离，不能互相干扰。关于事务的隔离性，可能不是特别好理解，这  里的并发事务是指两个事务操作了同一份数据的情况；而对于并发事务操作同一份数据的隔离性问题，则是要求不  能出现脏读、幻读的情况，即事务A不能读取事务B还没有提交的数据，或者在事务A读取数据进行更新操作时，不  允许事务B率先更新掉这条数据。而为了解决这个问题，常用的手段就是加锁了，对于数据库来说就是通过数据库  的相关锁机制来保证。

Durablity（持久性）：事务完成后，对数据库的更改是永久保存的。

#### 1.6.2 什么是分布式事务

其实分布式事务从实质上看与数据库事务的概念是一致的，既然是事务也就需要满足事务的基本特性（ACID），只  是分布式事务相对于本地事务而言其表现形式有很大的不同

 

 

## 1.7 分布式理论：一致性协议 2PC

### 1.7.1 什么是 2PC

2PC （ Two-Phase Commit缩写）即两阶段提交协议，是将整个事务流程分为两个阶段，准备阶段（Prepare phase）、提交阶段（commit phase），2是指两个阶段，P是指准备阶段，C是指提交阶段。

在计算机中部分关系数据库如Oracle、MySQL支持两阶段提交协议.


![img](Design.assets/wps44-1610369722361.png)



两个阶段过程：

1. 准备阶段（Prepare phase）：事务管理器给每个参与者发送Prepare消息，每个数据库参与者在本地执行事务，并写本地的Undo/Redo日志，此时事务没有提交。 （Undo日志是记录修改前的数据，用于数据库回滚， Redo日志是记录修改后的数据，用于提交事务后写入数 据文件）

2. 提交阶段（commit   phase）：如果事务管理器收到了参与者的执行失败或者超时消息时，直接给每个参与者发送回滚(Rollback)消息；否则，发送提交(Commit)消息；参与者根据事务管理器的指令执行提交或者回滚操   作，并释放事务处理过程中使用的锁资源。注意:必须在最后阶段释放锁资源。



协议说明：

顾名思义，二阶段提交就是将事务的提交过程分成了两个阶段来进行处理。流程如下：

### 1.7.2 2PC执行流程

成功执行事务事务提交流程


![img](Design.assets/wps45-1610369724596.png)


```
阶段一:
1.事务询问
协调者向所有的参与者发送事务内容，询问是否可以执行事务提交操作，并开始等待各参与者的响应。
2.执行事务 (写本地的Undo/Redo日志)
3.各参与者向协调者反馈事务询问的响应总结: 各个参与者进行投票是否让事务进行.
```


Tip: 什么是Ack


```
ACK 确认字符，在数据通信中，接收站发给发送站的一种传输类控制字符。表示发来的数据已确认接收无误。
```


```
阶段二:
1.发送提交请求：
协调者向所有参与者发出 commit 请求。
2.事务提交：
参与者收到 commit 请求后，会正式执行事务提交操作，并在完成提交之后释放整个事务执行期间占用的事务资源。
3.反馈事务提交结果：
参与者在完成事务提交之后，向协调者发送 Ack 信息。
4.完成事务：
协调者接收到所有参与者反馈的 Ack 信息后，完成事务。
```

 

中断事务步骤如下：

假如任何一个参与者向协调者反馈了No响应，或者在等待超时之后，协调者尚无法接收到所有参与者的反馈响应，  那么就会中断事务



![img](Design.assets/wps49.png)![img](Design.assets/wps50.png)![img](Design.assets/wps51-1610369726995.png) 

阶段一:


```
1.事务询问
协调者向所有的参与者发送事务内容，询问是否可以执行事务提交操作，并开始等待各参与者的响应。
2.执行事务 (写本地的Undo/Redo日志)
3.各参与者向协调者反馈事务询问的响应总结: 各个参与者进行投票是否让事务进行.
```


阶段二


```
1.发送回滚请求：
协调者向所有参与者发出 Rollback 请求。
2.事务回滚：
参与者接收到 Rollback 请求后，会利用其在阶段一中记录的 Undo 信息来执行事务回滚操作，并在完成回滚之后释放在整个事务执行期间占用的资源。
3.反馈事务回滚结果：
参与者在完成事务回滚之后，向协调者发送 Ack 信息。
4.中断事务：
协调者接收到所有参与者反馈的 Ack 信息后，完成事务中断。

从上面的逻辑可以看出，二阶段提交就做了2个事情：投票，执行。
```

 

### 1.7.3 2PC 优点缺点

> 优点

原理简单，实现方便

> 缺点

同步阻塞，单点问题，数据不一致，过于保守

- 同步阻塞：

二阶段提交协议存在最明显也是最大的一个问题就是同步阻塞，在二阶段提交的执行过程中，所有参与该事务操作  的逻辑都处于阻塞状态，也就是说，各个参与者在等待其他参与者响应的过程中，无法进行其他操作。这种同步阻  塞极大的限制了分布式系统的性能。

- 单点问题：

协调者在整个二阶段提交过程中很重要，如果协调者在提交阶段出现问题，那么整个流程将无法运转，更重要的  是：其他参与者将会处于一直锁定事务资源的状态中，而无法继续完成事务操作。

- 数据不一致：

假设当协调者向所有的参与者发送 commit 请求之后，发生了局部网络异常或者是协调者在尚未发送完所有 commit

请求之前自身发生了崩溃，导致最终只有部分参与者收到了 commit 请求。这将导致严重的数据不一致问题。

- 过于保守：

如果在二阶段提交的提交询问阶段中，参与者出现故障而导致协调者始终无法获取到所有参与者的响应信息的话，  这时协调者只能依靠其自身的超时机制来判断是否需要中断事务，显然，这种策略过于保守。换句话说，二阶段提  交协议没有设计较为完善的容错机制，任意一个节点失败都会导致整个事务的失败。


## 1.8 分布式理论：一致性协议 3PC

### 1.8.1 什么是三阶段提交

3PC，全称 “three phase commit”，是 2PC 的改进版，将 2PC 的 “提交事务请求” 过程一分为二，共形成了由

CanCommit、PreCommit和doCommit三个阶段组成的事务处理协议。


![img](Design.assets/wps61-1610369730452.png)


### 1.8.2 阶段一：CanCommit

第一个阶段： CanCommit

① 事务询问

协调者向所有的参与者发送一个包含事务内容的canCommit请求，询问是否可以执行事务提交操作，并开始等待各   参与者的响应。

② 各参与者向协调者反馈事务询问的响应

参与者在接收到来自协调者的包含了事务内容的canCommit请求后，正常情况下，如果自身认为可以顺利执行事  务，则反馈Yes响应，并进入预备状态，否则反馈No响应。


![img](Design.assets/wps62-1610369733267.png)

 

### 1.8.3 阶段二：PreCommit

协调者在得到所有参与者的响应之后，会根据结果有2种执行操作的情况：执行事务预提交，或者中断事务  假如所有参与反馈的都是Yes，那么就会执行事务预提交。

1. 执行事务预提交分为 3 个步骤

① 发送预提交请求：

协调者向所有参与者节点发出preCommit请求，并进入prepared阶段。

② 事务预提交：

参与者接收到preCommit请求后，会执行事务操作，并将Undo和Redo信息记录到事务日志中。

③ 各参与者向协调者反馈事务执行的结果：

若参与者成功执行了事务操作，那么反馈Ack


若任一参与者反馈了No响应，或者在等待超时后，协调者尚无法接收到所有参与者反馈，则中断事务

2. 中断事务也分为2个步骤：

① 发送中断请求：

协调者向所有参与者发出abort请求。

② 中断事务：

无论是收到来自协调者的abort请求或者等待协调者请求过程中超时，参与者都会中断事务

 

### 1.8.4 阶段三：do Commit


![img](Design.assets/wps63-1610369736220.png)

 

该阶段做真正的事务提交或者完成事务回滚，所以就会出现两种情况：

1. 执行事务提交

① 发送提交请求：

进入这一阶段，假设协调者处于正常工作状态，并且它接收到了来自所有参与者的Ack响应，那么他将从预提交状  态转化为提交状态，并向所有的参与者发送doCommit请求。

② 事务提交：

参与者接收到doCommit请求后，会正式执行事务提交操作，并在完成提交之后释放整个事务执行过程中占用的事  务资源。

③ 反馈事务提交结果：



参与者在完成事务提交后，向协调者发送Ack响应。

④ 完成事务：

协调者接收到所有参与者反馈的Ack消息后，完成事务。

 

2. 中断事务

① 发送中断请求：协调者向所有的参与者节点发送abort请求。

②  事务回滚：参与者收到abort请求后，会根据记录的Undo信息来执行事务回滚，并在完成回滚之后释放整个事务执行期间占用的资源。

③ 反馈事务回滚结果：参与者在完成事务回滚后，向协调者发送Ack消息。

④ 中断事务：协调者接收到所有参与者反馈的Ack消息后，中断事务。注意：一旦进入阶段三，可能会出现 2 种故障：

1. 协调者出现问题

2. 协调者和参与者之间的网络故障

如果出现了任一一种情况，最终都会导致参与者无法收到 doCommit 请求或者  abort  请求，针对这种情况，参与者都会在等待超时之后，继续进行事务提交

 

### 1.8.5 2PC对比3PC

1.首先对于协调者和参与者都设置了超时机制（在2PC中，只有协调者拥有超时机制，即如果在一定时间内没有收   到参与者的消息则默认失败）,主要是避免了参与者在长时间无法与协调者节点通讯（协调者挂掉了）的情况下，无  法释放资源的问题，因为参与者自身拥有超时机制会在超时后，自动进行本地commit从而进行释放资源。而这种机  制也侧面降低了整个事务的阻塞时间和范围。 2.通过CanCommit、PreCommit、DoCommit三个阶段的设计，相较于2PC而言，多设置了一个缓冲阶段保证了在最后提交阶段之前各参与节点的状态是一致的  。  3.PreCommit是一个缓冲，保证了在最后提交阶段之前各参与节点的状态是一致的。

问题：3PC协议并没有完全解决数据不一致问题。



## 1.9 分布式理论：一致性算法 Paxos


### 1.9.1 什么是Paxos算法


```
Paxos算法是Lamport提出的一种基于消息传递的分布式一致性算法，使其获得2013年图灵奖。
```


![img](Design.assets/wps67-1610369739911.png) 



 

Paxos由Lamport于1998年在《The Part-Time Parliament》论文中首次公开，最初的描述使用希腊的一个小岛Paxos 作为比喻，描述了Paxos小岛中通过决议的流程，并以此命名这个算法，但是这个描述理解起来比较有挑战性。后  来在2001年，Lamport觉得同行不能理解他的幽默感，于是重新发表了朴实的算法描述版本《Paxos Made Simple》

自Paxos问世以来就持续垄断了分布式一致性算法，Paxos这个名词几乎等同于分布式一致性。Google的很多大型分   布式系统都采用了Paxos算法来解决分布式一致性问题，如Chubby、Megastore以及Spanner等。开源的ZooKeeper，   以及MySQL 5.7推出的用来取代传统的主从复制的MySQL Group Replication等纷纷采用Paxos算法解决分布式一致性问题。

然而，Paxos的最大特点就是难，不仅难以理解，更难以实现。

### 1.9.2 Paxos 解决了什么问题

答：解决了分布式系统一致性问题。

```
分布式系统才用多副本进行存储数据 , 如果对多个副本执行序列不控制, 那多个副本执行更新操作,由于网络延迟 超时等故障到值各个副本的数据不一致.
我们希望每个副本的执行序列是 [ op1 op2 op3 .... opn ] 不变的, 相同的.
Paxos 一次来确定不可变变量 opi的取值 , 每次确定完Opi之后,各个副本执行opi操作,一次类推。

结论： Paxos算法需要解决的问题就是如何在一个可能发生上述异常的分布式系统中，快速且正确地在集群内部对某个数据的值达成一致.

注：这里某个数据的值并不只是狭义上的某个数，它可以是一条日志，也可以是一条命令（command）。根据应用场景不同，某个数据的值有不同的含义
```


![img](Design.assets/wps69-1610369742236.png) 

 

我们假设一种情况，在一个集群环境中，要求所有机器上的状态是一致的，其中有2台机器想修改某个状态，机器

A 想把状态改为 A，机器 B 想把状态改为 B，那么到底听谁的呢？

有的同学会想到，可以像 2PC，3PC 一样引入一个协调者，谁先到，听谁的


![img](Design.assets/wps70.jpg)

 

但是如果，协调者宕机了呢？

所以需要对协调者也做备份，也要做集群。这时候，问题来了，这么多协调者，听谁的呢？



![img](Design.assets/wps71-1610369748385.png) 

 

Paxos 算法就是为了解决这个问题而生的

 

### 1.9.3 Paxos相关概念

首先一个很重要的概念叫提案（Proposal）。最终要达成一致的value就在提案里。  提案 (Proposal)：Proposal信息包括提案编号 (Proposal ID) 和提议的值 (Value)

在Paxos算法中，有如下角色：

- Client：客户端

  客户端向分布式系统发出请求 ，并等待响应 。例如，对分布式文件服务器中文件的写请求。

- Proposer：提案发起者

  提案者提倡客户请求，试图说服Acceptor对此达成一致，并在发生冲突时充当协调者以推动协议向前发  展

- Acceptor：决策者，可以批准提案

  Acceptor可以接受（accept）提案；如果某个提案被选定（chosen），那么该提案里的value就被选定了

- Learners：最终决策的学习者

  学习者充当该协议的复制因素



![img](Design.assets/wps80-1610369752203.png) 

 

### 1.9.4 问题描述

假设有一组可以提出提案的进程集合，那么对于一个一致性算法需要保证以下几点：

- 在这些被提出的提案中，只有一个会被选定

- 如果没有提案被提出，就不应该有被选定的提案。

- 当一个提案被选定后，那么所有进程都应该能学习（learn）到这个被选定的value

### 1.9.5 推导过程

#### 1.9.5.1 最简单的方案——只有一个Acceptor

假设只有一个Acceptor（可以有多个Proposer），只要Acceptor接受它收到的第一个提案，则该提案被选定，该提案   里的value就是被选定的value。这样就保证只有一个value会被选定。

但是，如果这个唯一的Acceptor宕机了，那么整个系统就无法工作了！ 因此，必须要有多个Acceptor！



![img](Design.assets/wps84-1610369756440.png) 

 

#### 1.9.5.2 多个Acceptor

多个Acceptor的情况如下图。那么，如何保证在多个Proposer和多个Acceptor的情况下选定一个value呢？


![img](Design.assets/wps85-1610369758742.png)




下面开始寻找解决方案。

首先我们希望即使只有一个Proposer提出了一个value，该value也最终被选定。  那么，就得到下面的约束：

```
P1：一个Acceptor必须接受它收到的第一个提案。
```

但是，这又会引出另一个问题：如果每个Proposer分别提出不同的value，发给不同的Acceptor。根据P1，Acceptor   分别接受自己收到的第一个提案，就导致不同的value被选定。出现了不一致。如下图：


![img](Design.assets/wps87-1610369761459.png)


刚刚是因为『一个提案只要被一个Acceptor接受，则该提案的value就被选定了』才导致了出现上面不一致的问题。  因此，我们需要加一个规定：


```
规定：一个提案被选定需要被半数以上的Acceptor接受
```

这个规定又暗示了：『一个Acceptor必须能够接受不止一个提案！』不然可能导致最终没有value被选定。比如上图  的情况。v1、v2、v3都没有被选定，因为它们都只被一个Acceptor的接受。

所以在这种情况下，我们使用一个全局的编号来标识每一个Acceptor批准的提案，当一个具有某value值的提案被半  数以上的Acceptor批准后，我们就认为该value被选定了.

根据上面的内容，我们现在虽然允许多个提案被选定，但必须保证所有被选定的提案都具有相同的value值。否则又  会出现不一致。

于是有了下面的约束：

```
P2：如果某个value为v的提案被选定了，那么每个编号更高的被选定提案的value必须也是v。
```


一个提案只有被Acceptor接受才可能被选定，因此我们可以把P2约束改写成对Acceptor接受的提案的约束P2a。

```
P2a：如果某个value为v的提案被选定了，那么每个编号更高的被Acceptor接受的提案的value必须也是v。
```

只要满足了P2a，就能满足P2。



![img](Design.assets/wps91-1610369764607.png) 

 

但是，考虑如下的情况：假设总的有5个Acceptor。Proposer2提出[M1,V1]的提案，Acceptor2~5（半数以上）均接受  了该提案，于是对于Acceptor2~5和Proposer2来讲，它们都认为V1被选定。Acceptor1刚刚从宕机状态恢复过来（之  前Acceptor1没有收到过任何提案），此时Proposer1向Acceptor1发送了[M2,V2]的提案（V2≠V1且M2>M1），对于Acceptor1来讲，这是它收到的第一个提案。根据P1（一个Acceptor必须接受它收到的第一个提案。）,Acceptor1必  须接受该提案！同时Acceptor1认为V2被选定。这就出现了两个问题：

1. Acceptor1认为V2被选定，Acceptor2~5和Proposer2认为V1被选定。出现了不一致。

2. V1被选定了，但是编号更高的被Acceptor1接受的提案[M2,V2]的value为V2，且V2≠V1。这就跟P2a（如果某个value为v的提案被选定了，那么每个编号更高的被Acceptor接受的提案的value必须也是v）矛盾了。

所以我们要对P2a约束进行强化！

P2a是对Acceptor接受的提案约束，但其实提案是Proposer提出来的，所有我们可以对Proposer提出的提案进行约   束。得到P2b：

```
P2b：如果某个value为v的提案被选定了，那么之后任何Proposer提出的编号更高的提案的value必须也是v。
```



由P2b可以推出P2a进而推出P2。

```
P2c：对于任意的Mn和Vn,如果提案[Mn,Vn]被提出，那么肯定存在一个由半数以上的Acceptor组成的集合S，满足以下两个条件       中的任意一个：

*要么S中每个Acceptor都没有接受过编号小于Mn的提案。
*要么S中所有Acceptor批准的所有编号小于Mn的提案中，编号最大的那个提案的value值为Vn
```

那么，如何确保在某个value为v的提案被选定后，Proposer提出的编号更高的提案的value都是v呢？  只要满足P2c即可：



从上面的内容，可以看出，从P1到P2c的过程其实是对一系列条件的逐步增强，如果需要证明这些条件可以保证一  致性，那么就可以进行反向推导：P2c =>P2b=>P2a=>P2,然后通过P2和P1来保证一致性


#### 1.9.5.3 Proposer生成提案

接下来来学习，在P2c的基础上如何进行提案的生成

这里有个比较重要的思想：Proposer生成提案之前，应该先去『学习』已经被选定或者可能被选定的value，然后以   该value作为自己提出的提案的value。如果没有value被选定，Proposer才可以自己决定value的值。这样才能达成一   致。这个学习的阶段是通过一个『Prepare请求』实现的。

于是我们得到了如下的提案生成算法：

1. Proposer选择一个新的提案编号N，然后向某个Acceptor集合（半数以上）发送请求，要求该集合中的每个Acceptor做出如下响应（response）

(a) Acceptor向Proposer承诺保证不再接受任何编号小于N的提案。

(b) 如果Acceptor已经接受过提案，那么就向Proposer反馈已经接受过的编号小于N的，但为最大编号的提案  的值。

我们将该请求称为编号为N的Prepare请求。

2. 如果Proposer收到了半数以上的Acceptor的响应，那么它就可以生成编号为N，Value为V的提案[N,V]。这里的V  是所有的响应中编号最大的提案的Value。如果所有的响应中都没有提案，那  么此时V就可以由Proposer自己选择。

生成提案后，Proposer将该提案发送给半数以上的Acceptor集合，并期望这些Acceptor能接受该提案。我们称  该请求为Accept请求。

 

#### 1.9.5.4 Acceptor接受提案

刚刚讲解了Paxos算法中Proposer的处理逻辑，怎么去生成的提案，下面来看看Acceptor是如何批准提案的

根据刚刚的介绍，一个Acceptor可能会受到来自Proposer的两种请求，分别是Prepare请求和Accept请求，对这两类   请求作出响应的条件分别如下

- Prepare请求：Acceptor可以在任何时候响应一个Prepare请求

- Accept请求：在不违背Accept现有承诺的前提下，可以任意响应Accept请求

因此，对Acceptor接受提案给出如下约束：

> P1a：一个Acceptor只要尚未响应过任何编号大于N的Prepare请求，那么他就可以接受这个编号为N的提案。

 

###### 1.9.5.5 算法优化

上面的内容中，分别从Proposer和Acceptor对提案的生成和批准两方面来讲解了Paxos算法在提案选定过程中的算法  细节，同时也在提案的编号全局唯一的前提下，获得了一个提案选定算法，接下来我们再对这个初步算法做一个小  优化，尽可能的忽略Prepare请求



![img](Design.assets/wps98-1610369769376.png) 


```
如果Acceptor收到一个编号为N的Prepare请求，在此之前它已经响应过编号大于N的Prepare请求。根据P1a，该Acceptor不可能接受编号为N的提案。因此，该Acceptor可以忽略编号为N的Prepare请求。
```


通过这个优化，每个Acceptor只需要记住它已经批准的提案的最大编号以及它已经做出Prepare请求响应的提案的最  大编号，以便出现故障或节点重启的情况下，也能保证P2c的不变性，而对于Proposer来说，只要它可以保证不会   产生具有相同编号的提案，那么就可以丢弃任意的提案以及它所有的运行时状态信息

 

1.9.6 Paxos算法描述

综合前面的讲解，我们来对Paxos算法的提案选定过程进行下总结，那结合Proposer和Acceptor对提案的处理逻辑，   就可以得到类似于两阶段提交的算法执行过程

Paxos算法分为两个阶段。具体如下：



![img](Design.assets/wps100-1610369772392.png) 

 

- 阶段一：

(a) Proposer选择一个提案编号N，然后向半数以上的Acceptor发送编号为N的Prepare请求。

(b) 如果一个Acceptor收到一个编号为N的Prepare请求，且N大于该Acceptor已经响应过的所有Prepare请求的编  号，那么它就会将它已经接受过的编号最大的提案（如果有的话）作为响应反馈给Proposer，同时该Acceptor  承诺不再接受任何编号小于N的提案。

- 阶段二：

(a) 如果Proposer收到半数以上Acceptor对其发出的编号为N的Prepare请求的响应，那么它就会发送一个针对[N,V]提案的Accept请求给半数以上的Acceptor。注意：V就是收到的响应中编号最大的提案的value，如果响应  中不包含任何提案，那么V就由Proposer自己决定。

(b) 如果Acceptor收到一个针对编号为N的提案的Accept请求，只要该Acceptor没有对编号大于N的Prepare请求  做出过响应，它就接受该提案。

当然，实际运行过程中，每一个Proposer都有可能产生多个提案，但只要每个Proposer都遵循如上所述的算法运   行，就一定能够保证算法执行的正确性

 

### 1.9.7 Learner学习被选定的value

刚刚我们介绍了如何来选定一个提案，下面我们再来看看如


![img](Design.assets/wps103-1610369775190.png) 

 

方案一：  Learner获取一个已经被选定的提案的前提是，该提案已经被半数以上的Acceptor批准，因此，最简单的做法就是一旦Acceptor批准了一个提案，就将该提案发送给所有的Learner

很显然，这种做法虽然可以让Learner尽快地获取被选定的提案，但是却需要让每个Acceptor与所有的Learner逐个进   行一次通信，通信的次数至少为二者个数的乘积


方案二：

另一种可行的方案是，我们可以让所有的Acceptor将它们对提案的批准情况，统一发送给一个特定的Learner（称为  主Learner）,  各个Learner之间可以通过消息通信来互相感知提案的选定情况，基于这样的前提，当主Learner被通知一个提案已经被选定时，它会负责通知其他的learner

在这种方案中，Acceptor首先会将得到批准的提案发送给主Learner,再由其同步给其他Learner.因此较方案一而言，  方案二虽然需要多一个步骤才能将提案通知到所有的learner，但其通信次数却大大减少了，通常只是Acceptor和Learner的个数总和，但同时，该方案引入了一个新的不稳定因素：主Learner随时可能出现故障


方案三：

在讲解方案二的时候，我们提到，方案二最大的问题在于主Learner存在单点问题，即主Learner随时可能出现故  障，因此，对方案二进行改进，可以将主Learner的范围扩大，即Acceptor可以将批准的提案发送给一个特定的

Learner集合，该集合中每个Learner都可以在一个提案被选定后通知其他的Learner。这个Learner集合中的Learner个   数越多，可靠性就越好，但同时网络通信的复杂度也就越高



### 1.9.8 如何保证Paxos算法的活性

根据前面的内容讲解，我们已经基本上了解了Paxos算法的核心逻辑，那接下来再来看看Paxos算法在实际过程中的   一些细节

活性：最终一定会发生的事情：最终一定要选定value


假设存在这样一种极端情况，有两个Proposer依次提出了一系列编号递增的提案，导致最终陷入死循环，没有value   被选定,具体流程如下:

![img](Design.assets/wps104-1610369779105.png)

 

解决：通过选取主Proposer，并规定只有主Proposer才能提出议案。这样一来只要主Proposer和过半的Acceptor能够   正常进行网络通信，那么但凡主Proposer提出一个编号更高的提案，该提案终将会被批准，这样通过选择一个主Proposer，整套Paxos算法就能够保持活性




## 1.10 分布式理论：一致性算法 Rah

1.10.1 什么是Rah 算法

首先说什么是 Rah 算法：Rah 是一种为了管理复制日志的一致性算法。

Rah提供了和Paxos算法相同的功能和性能，但是它的算法结构和Paxos不同。Rah算法更加容易理解并且更容易构建   实际的系统。

Rah将一致性算法分解成了3模块

1. 领导人选举

2. 日志复制

3. 安全性

Rah算法分为两个阶段，首先是选举过程，然后在选举出来的领导人带领进行正常操作，比如日志复制等。

 

### 1.10.2 领导人Leader选举



Rah  通过选举一个领导人，然后给予他全部的管理复制日志的责任来实现一致性。在Rah中，任何时候一个服务器都可以扮演下面的角色之一：

- 领导者(leader)：处理客户端交互，日志复制等动作，一般一次只有一个领导者

- 候选者(candidate)：候选者就是在选举过程中提名自己的实体，一旦选举成功，则成为领导者

- 跟随者(follower)：类似选民，完全被动的角色，这样的服务器等待被通知投票

而影响他们身份变化的则是 选举。


![img](Design.assets/wps109-1610369782899.png)

 

Rah使用心跳机制来触发选举。当server启动时，初始状态都是follower。每一个server都有一个定时器，超时时间   为election   timeout（一般为150-300ms），如果某server没有超时的情况下收到来自领导者或者候选者的任何消息， 定时器重启，如果超时，它就开始一次选举。

[http://thesecretlivesofdata.com/rah/](http://thesecretlivesofdata.com/raft/) 动画演示下面用图示展示这个过程：

初始状态下集群中的所有节点都处于 follower 状态。


![img](Design.assets/wps110.jpg)


➢ 某一时刻，其中的一个 follower 由于没有收到 leader 的 heartbeat 率先发生 election timeout 进而发起选举。


![img](Design.assets/wps111.jpg)


➢ 只要集群中超过半数的节点接受投票，candidate 节点将成为即切换 leader 状态。


![img](Design.assets/wps112.jpg)


➢ 成为 leader 节点之后，leader 将定时向 follower 节点同步日志并发送 heartbeat。


![img](Design.assets/wps113.jpg)


### 1.10.3 节点异常

集群中各个节点的状态随时都有可能发生变化。从实际的变化上来分类的话，节点的异常大致可以分为四种类型：

- leader 不可用；

- follower 不可用；

- 多个 candidate 或多个 leader； 

- 新节点加入集群。

#### 1.10.3.1 leader 不可用

下面将说明当集群中的 leader 节点不可用时，rah 集群是如何应对的。

➢ 一般情况下，leader 节点定时发送 heartbeat 到 follower 节点。



![img](Design.assets/wps118.jpg) 

 

➢ 由于某些异常导致 leader 不再发送 heartbeat ，或 follower 无法收到 heartbeat 。


![img](Design.assets/wps119.jpg)



➢ 当某一 follower 发生 election timeout 时，其状态变更为 candidate，并向其他 follower 发起投票。


![img](Design.assets/wps120.jpg)



➢ 当超过半数的 follower 接受投票后，这一节点将成为新的 leader，leader 的步进数加 1 并开始向 follower 同步日志。



![img](Design.assets/wps121.jpg) 

 

➢ 当一段时间之后，如果之前的 leader 再次加入集群，则两个 leader 比较彼此的步进数，步进数低的 leader 将切换自己的状态为 follower。


![img](Design.assets/wps122.jpg)



➢ 较早前 leader 中不一致的日志将被清除，并与现有 leader 中的日志保持一致。


![img](Design.assets/wps123.jpg)



#### 1.10.3.2 follower 节点不可用

follower 节点不可用的情况相对容易解决。因为集群中的日志内容始终是从 leader  节点同步的，只要这一节点再次加入集群时重新从 leader 节点处复制日志即可。

➢ 集群中的某个 follower 节点发生异常，不再同步日志以及接收 heartbeat。



![img](Design.assets/wps124.jpg) 

 

➢ 经过一段时间之后，原来的 follower 节点重新加入集群。


![img](Design.assets/wps125.jpg)



➢ 这一节点的日志将从当时的 leader 处同步。


![img](Design.assets/wps126.jpg)



#### 1.10.3.3 多个 candidate 或多个 leader

在集群中出现多个 candidate 或多个 leader 通常是由于数据传输不畅造成的。出现多个 leader 的情况相对少见，但多个 candidate 比较容易出现在集群节点启动初期尚未选出 leader 的“混沌”时期。

➢ 初始状态下集群中的所有节点都处于 follower 状态。



![img](Design.assets/wps127.jpg) 

 

➢ 两个节点同时成为 candidate 发起选举。


![img](Design.assets/wps128.jpg)



➢ 两个 candidate 都只得到了少部分 follower 的接受投票。


![img](Design.assets/wps129.jpg)



➢ candidate 继续向其他的 follower 询问。



![img](Design.assets/wps130.jpg) 

 

➢ 由于一些 follower 已经投过票了，所以均返回拒绝接受。


![img](Design.assets/wps131.jpg)



➢ candidate 也可能向一个 candidate 询问投票。


![img](Design.assets/wps132.jpg)



➢ 在步进数相同的情况下，candidate 将拒绝接受另一个 candidate 的请求。



![img](Design.assets/wps133.jpg) 

 

➢ 由于第一次未选出 leader，candidate 将随机选择一个等待间隔（150ms ~ 300ms）再次发起投票。


![img](Design.assets/wps134.jpg)



➢ 如果得到集群中半数以上的 follower 的接受，这一 candidate 将成为 leader。


![img](Design.assets/wps135.jpg)



➢ 稍后另一个 candidate 也将再次发起投票。



![img](Design.assets/wps136.jpg) 

 

➢ 由于集群中已经选出 leader，candidate 将收到拒绝接受的投票。


![img](Design.assets/wps137.jpg)




➢ 在被多数节点拒绝之后，并已知集群中已存在 leader 后，这一 candidate 节点将终止投票请求、切换为

follower，从 leader 节点同步日志。


![img](Design.assets/wps138.jpg)




#### 1.10.4 日志复制（保证数据一致性）

日志复制的过程

Leader选出后，就开始接收客户端的请求。Leader把请求作为日志条目（Log entries）加入到它的日志中，然后并行的向其他服务器发起  AppendEntries  RPC复制日志条目。当这条日志被复制到大多数服务器上，Leader将这条日志应用到它的状态机并向客户端返回执行结果。



下图表示了当一个客户端发送一个请求给领导者，随后领导者复制给跟随者的整个过程。


![img](Design.assets/wps139-1610369810807.png)






4 个步骤：

（1）客户端的每一个请求都包含被复制状态机执行的指令。

（2）把这个指令作为一条新的日志条目添加到日志中，然后并行发起 RPC  给其他的服务器，让他们复制这条信息。

（3）跟随者响应ACK,如果 follower 宕机或者运行缓慢或者丢包，leader会不断的重试，直到所有的 follower 最终都复制了所有的日志条目。

（4）通知所有的Follower提交日志，同时领导人提交这条日志到自己的状态机中，并返回给客户端。

可以看到，直到第四步骤，整个事务才会达成。中间任何一个步骤发生故障，都不会影响日志一致性。

 


# 第2章 分布式系统设计策略

分布式系统本质是通过低廉的硬件攒在一起以获得更好的吞吐量、性能以及可用性等。 在分布式环境下，有几个问题是普遍关心的，我们称之为设计策略：

- 如何检测当前节点还活着？ 
- 如何保障高可用？
- 容错处理
- 负载均衡


## 2.1 心跳检测



 

在分布式环境中，我们提及过存在非常多的节点（Node），其实质是这些节点分担任务的运行、计算或者程序逻辑  处理。那么就有一个非常重要的问题，如何检测一个节点出现了故障乃至无法工作了？

通常解决这一问题是采用心跳检测的手段，如同通过仪器对病人进行一些检测诊断一样。

心跳顾名思义，就是以固定的频率向其他节点汇报当前节点状态的方式。收到心跳，一般可以认为一个节点和现在  的网络拓扑是良好的。当然，心跳汇报时，一般也会携带一些附加的状态、元数据信息，以便管理

如图所示，Client请求Server，Server转发请求到具体的Node获取请求结果。Server需要与三个Node节点保持心跳连   接，确保Node可以正常工作。



|      |                                                              |
| ---- | ------------------------------------------------------------ |
|      | ![img](file:///C:\Users\周壮\AppData\Local\Temp\ksohtml9152\wps149.png) |

 



 

 

若Server没有收到Node3的心跳时，Server认为Node3失联。但是失联是失去联系，并不确定是否是Node3故障，有   可能是Node3处于繁忙状态，导致调用检测超时；也有可能是Server与Node3之间链路出现故障或闪断。所以心跳不  是万能的，收到心跳可以确认节点正常，但是收不到心跳也不能认为该节点就已经宣告“死亡”。此时，可以通过一  些方法帮助Server做决定： 周期检测心跳机制、累计失效检测机制。

 

周期检测心跳机制

Server端每间隔 t 秒向Node集群发起监测请求，设定超时时间，如果超过超时时间，则判断“死亡”。累计失效检测机制

在周期检测心跳机制的基础上，统计一定周期内节点的返回情况（包括超时及正确返回），以此计算节点的“死  亡”概率。另外，对于宣告“濒临死亡”的节点可以发起有限次数的重试，以作进一步判断。



通过周期检测心跳机制、累计失效检测机制可以帮助判断节点是否“死亡”，如果判断“死亡”，可以把该节点踢出集  群

 

 

## 2.2 ![img](Design.assets/wps150.png)高可用设计

高可用(High Availability)是系统架构设计中必须考虑的因素之一,通常是指,经过设计来减少系统不能提供服务的时间

.



|      |                                                              |
| ---- | ------------------------------------------------------------ |
|      | ![img](file:///C:\Users\周壮\AppData\Local\Temp\ksohtml9152\wps151.png) |

 



 

系统高可用性的常用设计模式包括三种：主备（Master-SLave）、互备（Active-Active）和集群（Cluster）模式。

\1. 主备模式   主备模式就是Active-Standby模式，当主机宕机时，备机接管主机的一切工作，待主机恢复正常后，按使用者的设定以自动（热备）或手动（冷备）方式将服务切换到主机上运行。在数据库部分，习惯称之为MS模式。 MS模式即Master/Slave模式，这在数据库高可用性方案中比较常用，如MySQL、Redis等就采用MS模式实现主从复    制。保证高可用，如图所示。



|      |                                  |
| ---- | -------------------------------- |
|      | ![img](Design.assets/wps152.jpg) |

 



 

MySQL之间数据复制的基础是二进制日志文件（binary log file）。一台MySQL数据库一旦启用二进制日志后，作为master，它的数据库中所有操作都会以“事件”的方式记录在二进制日志中，其他数据库作为slave通过一个I/O线程与  主服务器保持通信，并监控master的二进制日志文件的变化，如果发现master二进制日志文件发生变化，则会把变  化复制到自己的中继日志中，然后slave的一个SQL线程会把相关的“事件”执行到自己的数据库中，以此实现从数据   库和主数据库的一致性，也就实现了主从复制。



\2. 互备模式   互备模式指两台主机同时运行各自的服务工作且相互监测情况。在数据库高可用部分，常见的互备是MM模式。MM模式即Multi-Master模式，指一个系统存在多个master，每个master都具有read-write能力，会根据时间 戳或业务逻辑合并版本。

我们使用过的、构建过的MySQL服务绝大多数都是Single-Master，整个拓扑中只有一个Master承担写请求。比如，  基于Master-Slave架构的主从复制，但是也存在由于种种原因，我们可能需要MySQL服务具有Multi-Master的特性，   希望整个拓扑中可以有不止一个Master承担写请求



|      |                                  |
| ---- | -------------------------------- |
|      | ![img](Design.assets/wps153.jpg) |

 



 

\3. 集群模式

集群模式是指有多个节点在运行，同时可以通过主控节点分担服务请求。如Zookeeper。集群模式需要解决主控节  点本身的高可用问题，一般采用主备模式。

 

 

## 2.3 ![img](Design.assets/wps154.png)容错性

容错顾名思义就是IT系统对于错误包容的能力

容错的处理是保障分布式环境下相应系统的高可用或者健壮性，一个典型的案例就是对于缓存穿透  问题的解决方案。

我们来具体看一下这个例子，如图所示



|      |                                                              |
| ---- | ------------------------------------------------------------ |
|      | ![img](file:///C:\Users\周壮\AppData\Local\Temp\ksohtml9152\wps155.png) |

 





问题描述：

我们在项目中使用缓存通常都是先检查缓存中是否存在，如果存在直接返回缓存内容，如果不存在就直接查询数据  库然后再缓存查询结果返回。这个时候如果我们查询的某一个数据在缓存中一直不存在，就会造成每一次请求都查  询DB，这样缓存就失去了意义，在流量大时，或者有人恶意攻击

如频繁发起为id为“-1”的条件进行查询，可能DB就挂掉了。那这种问题有什么好办法解决呢？

一个比较巧妙的方法是，可以将这个不存在的key预先设定一个值。比如，key=“null”。在返回这个null值的时候，  我们的应用就可以认为这是不存在的key，那我们的应用就可以决定是否继续等待访问，还是放弃掉这次操作。如  果继续等待访问，过一个时间轮询点后，再次请求这个key，如果取到的值不再是null，则可以认为这时候key有值  了，从而避免了透传到数据库，把大量的类似请求挡在了缓存之中。

 

 

 

## 2.4 ![img](Design.assets/wps156.png)负载均衡

负载均衡：其关键在于使用多台集群服务器共同分担计算任务，把网络请求及计算分配到集群可用的不同服务器节  点上，从而达到高可用性及较好的用户操作体验。

如图，不同的用户User1、User2、User3访问应用，通过负载均衡器分配到不同的节点。



|      |                                                              |
| ---- | ------------------------------------------------------------ |
|      | ![img](file:///C:\Users\周壮\AppData\Local\Temp\ksohtml9152\wps157.png) |

 



负载均衡器有硬件解决方案，也有软件解决方案。硬件解决方案有著名的F5，软件有LVS、HAProxy、Nginx等。   以Nginx为例，负载均衡有以下几种策略：

· 轮询：即Round Robin，根据Nginx配置文件中的顺序，依次把客户端的Web请求分发到不同的后端服务器。

· 最少连接：当前谁连接最少，分发给谁。

· IP地址哈希：确定相同IP请求可以转发给同一个后端节点处理，以方便session保持。

· 基于权重的负载均衡：配置Nginx把请求更多地分发到高配置的后端服务器上，把相对较少的请求分发到低配服务  器。

**
**

# 第3章 分布式架构网络通信

在分布式服务框架中，一个最基础的问题就是远程服务是怎么通讯的，在Java领域中有很多可实现远程通讯的技  术，例如：RMI、Hessian、SOAP、ESB和JMS等，它们背后到底是基于什么原理实现的呢

 

## 3.1 ![img](Design.assets/wps158.png)基本原理

要实现网络机器间的通讯，首先得来看看计算机系统网络通信的基本原理，在底层层面去看，网络通信需要做的就  是将流从一台计算机传输到另外一台计算机，基于传输协议和网络IO来实现，其中传输协议比较出名的有tcp、udp  等等，tcp、udp都是在基于Socket概念上为某类应用场景而扩展出的传输协议，网络IO，主要有bio、nio、aio三种   方式，所有的分布式应用通讯都基于这个原理而实现，只是为了应用的易用，各种语言通常都会提供一些更为贴近  应用易用的应用层协议。

 

 

3.2 

|      |                                  |
| ---- | -------------------------------- |
|      | ![img](Design.assets/wps159.png) |

什么是RPC



RPC全称为remote procedure call，即远程过程调用。

借助RPC可以做到像本地调用一样调用远程服务，是一种进程间的通信方式

比如两台服务器A和B，A服务器上部署一个应用，B服务器上部署一个应用，A服务器上的应用想调用B服务器上的   应用提供的方法，由于两个应用不在一个内存空间，不能直接调用，所以需要通过网络来表达调用的语义和传达调  用的数据。

需要注意的是RPC并不是一个具体的技术，而是指整个网络远程调用过程。

 

RPC架构

一个完整的RPC架构里面包含了四个核心的组件，分别是Client，Client Stub，Server以及Server Stub，这个Stub可以理解为存根。

![img](Design.assets/wps160.png)客户端(Client)，服务的调用方。

![img](Design.assets/wps161.png)客户端存根(Client   Stub)，存放服务端的地址消息，再将客户端的请求参数打包成网络消息，然后通过网络远程发送给服务方。

![img](Design.assets/wps162.png)服务端(Server)，真正的服务提供者。

![img](Design.assets/wps163.png)服务端存根(Server Stub)，接收客户端发送过来的消息，将消息解包，并调用本地的方法。



![img](file:///C:\Users\周壮\AppData\Local\Temp\ksohtml9152\wps164.png) 



|      |                                  |
| ---- | -------------------------------- |
|      | ![img](Design.assets/wps165.jpg) |

 





(1) 客户端（client）以本地调用方式（即以接口的方式）调用服务；

(2) 客户端存根（client  stub）接收到调用后，负责将方法、参数等组装成能够进行网络传输的消息体（将消息体对象序列化为二进制）；

(3) 客户端通过sockets将消息发送到服务端；

(4) 服务端存根( server stub）收到消息后进行解码（将消息对象反序列化）；

(5) 服务端存根(  server stub）根据解码结果调用本地的服务；

(6) 本地服务执行并将结果返回给服务端存根(  server stub）；

(7) 服务端存根( server stub）将返回结果打包成消息（将结果消息对象序列化）；

(8) 服务端（server）通过sockets将消息发送到客户端；

(9) 客户端存根（client stub）接收到结果消息，并进行解码（将结果消息发序列化）；

(10) 客户端（client）得到最终结果。

RPC的目标是要把2、3、4、7、8、9这些步骤都封装起来。

注意：无论是何种类型的数据，最终都需要转换成二进制流在网络上进行传输，数据的发送方需要将对象转换为二  进制流，而数据的接收方则需要把二进制流再恢复为对象。

在java中RPC框架比较多，常见的有Hessian、gRPC、Thrih、HSF (High Speed Service Framework)、Dubbo 等，其实对 于RPC框架而言，核心模块 就是通讯和序列化

 

 

### 3.3 RMI



|      |                                  |
| ---- | -------------------------------- |
|      | ![img](Design.assets/wps166.png) |

 



#### 3.3.1 简介

Java RMI 指的是远程方法调用 (Remote Method Invocation),是java原生支持的远程调用 ,采用JRMP（Java Remote Messageing protocol）作为通信协议，可以认为是纯java版本的分布式远程调用解决方案，  RMI主要用于不同虚拟机之间的通信，这些虚拟机可以在不同的主机上、也可以在同一个主机上，这里的通信可以理解为一个虚拟机上的对  象调用另一个虚拟机上对象的方法。

 

\1. 客户端：

1） 存根/桩(Stub)：远程对象在客户端上的代理；

2） 远程引用层(Remote Reference Layer)：解析并执行远程引用协议；

3） 传输层(Transport)：发送调用、传递远程方法参数、接收远程方法执行结果。

\2. 服务端：

1） 骨架(Skeleton)：读取客户端传递的方法参数，调用服务器方的实际对象方法， 并接收方法执行后的返回值；

2） 远程引用层(Remote Reference Layer)：处理远程引用后向骨架发送远程方法调用；



3） 传输层(Transport)：监听客户端的入站连接，接收并转发调用到远程引用层。

\3. 注册表(Registry)：以URL形式注册远程对象，并向客户端回复对远程对象的引用。



|      |                                  |
| ---- | -------------------------------- |
|      | ![img](Design.assets/wps167.jpg) |

 



远程调用过程：



|      |                                                              |
| ---- | ------------------------------------------------------------ |
|      | ![img](file:///C:\Users\周壮\AppData\Local\Temp\ksohtml9152\wps168.png) |

 



结果返回过程：



|      |                                                              |
| ---- | ------------------------------------------------------------ |
|      | ![img](file:///C:\Users\周壮\AppData\Local\Temp\ksohtml9152\wps169.png) |

 



 

3.3.2 开发流程

1. 服务端：



![img](file:///C:\Users\周壮\AppData\Local\Temp\ksohtml9152\wps170.png)

2. 客户端：



|      |                                                              |
| ---- | ------------------------------------------------------------ |
|      | ![img](file:///C:\Users\周壮\AppData\Local\Temp\ksohtml9152\wps171.png) |

 



 

 

#### 3.3.3 代码实现

\1. 创建远程接口



|      |                                                              |
| ---- | ------------------------------------------------------------ |
|      | ![img](file:///C:\Users\周壮\AppData\Local\Temp\ksohtml9152\wps172.png) |

 



其中有一个引用对象作为参数



|      |                                                              |
| ---- | ------------------------------------------------------------ |
|      | ![img](file:///C:\Users\周壮\AppData\Local\Temp\ksohtml9152\wps173.png) |

 



\2. 实现远程服务对象



![img](file:///C:\Users\周壮\AppData\Local\Temp\ksohtml9152\wps174.png)

\3. 服务端程序



|      |                                                              |
| ---- | ------------------------------------------------------------ |
|      | ![img](file:///C:\Users\周壮\AppData\Local\Temp\ksohtml9152\wps175.png) |

 





![img](file:///C:\Users\周壮\AppData\Local\Temp\ksohtml9152\wps176.png)

\4. 客户端程序



|      |                                                              |
| ---- | ------------------------------------------------------------ |
|      | ![img](file:///C:\Users\周壮\AppData\Local\Temp\ksohtml9152\wps177.png) |

 



\5. 启动服务端程序

\6. 客户端调用

服务端：



|      |                                  |
| ---- | -------------------------------- |
|      | ![img](Design.assets/wps178.jpg) |

 



客户端



![img](Design.assets/wps179.jpg) 

 

 

 

 

### 3.4 ![img](Design.assets/wps180.png)BIO、NIO、AIO

#### 3.4.1 同步和异步

同步（synchronize）、异步（asychronize）是指应用程序和内核的交互而言的.   同步：

指用户进程触发IO操作等待或者轮训的方式查看IO操作是否就绪。



|      |                                  |
| ---- | -------------------------------- |
|      | ![img](Design.assets/wps181.png) |

 



 

异步：

当一个异步进程调用发出之后，调用者不会立刻得到结果。而是在调用发出之后，被调用者通过状态、通知来通知  调用者，或者通过回调函数来处理这个调用。



|      |                                                              |
| ---- | ------------------------------------------------------------ |
|      | ![img](file:///C:\Users\周壮\AppData\Local\Temp\ksohtml9152\wps182.png) |

使用异步IO时，Java将IO读写委托给OS处理，需要将数据缓冲区地址和大小传给OS，OS需要支持异步IO操作    举个例子：



 

 

#### 3.4.2 阻塞和非阻塞

阻塞和非阻塞是针对于进程访问数据的时候,根据IO操作的就绪状态来采取不同的方式.

简单点说就是一种读写操作方法的实现方式. 阻塞方式下读取和写入将一直等待,  而非阻塞方式下,读取和写入方法会理解返回一个状态值.

举个例子： 阻塞:

ATM机排队取款，你只能等待排队取款（使用阻塞IO的时候，Java调用会一直阻塞到读写完成才返回。）

非阻塞：



柜台取款，取个号，然后坐在椅子上做其他事，等广播通知，没到你的号你就不能去，但你可以不断的问大堂经理  排到了没有。（使用非阻塞IO时，如果不能读写Java调用会马上返回，当IO事件分发器会通知可读写时再继续进行   读写，不断循环直到读写完成）

 



|      |                                                              |
| ---- | ------------------------------------------------------------ |
|      | ![img](file:///C:\Users\周壮\AppData\Local\Temp\ksohtml9152\wps183.png) |

例子



 

 

 

 

 

 

 

 

##### 3.4.3 BIO

同步阻塞IO。B代表blocking

服务器实现模式为一个连接一个线程，即客户端有连接请求时服务器端就需要启动一个线程进行处理，如果这个连  接不做任何事情会造成不必要的线程开销，当然可以通过线程池机制改善。

适用场景：Java1.4之前唯一的选择，简单易用但资源开销太高



![img](file:///C:\Users\周壮\AppData\Local\Temp\ksohtml9152\wps184.png) 

 

 

服务端代码



|      |                                                              |
| ---- | ------------------------------------------------------------ |
|      | ![img](file:///C:\Users\周壮\AppData\Local\Temp\ksohtml9152\wps185.png) |

 





![img](Design.assets/wps186.png)

 

客户端代码



|      |                                                              |
| ---- | ------------------------------------------------------------ |
|      | ![img](file:///C:\Users\周壮\AppData\Local\Temp\ksohtml9152\wps187.png) |

 



执行结果:



|      |                                  |
| ---- | -------------------------------- |
|      | ![img](Design.assets/wps188.jpg) |

![img](Design.assets/wps189.jpg) 



 

 

 

##### 3.4.4 NIO

3.4.4.1 NIO介绍

同步非阻塞IO （non-blocking IO / new io）是指JDK 1.4 及以上版本。

服务器实现模式为一个请求一个通道，即客户端发送的连接请求都会注册到多路复用器上，多路复用器轮询到连接  有IO请求时才启动一个线程进行处理。

通道（Channels）



NIO 新引入的最重要的抽象是通道的概念。Channel 数据连接的通道。 数据可以从Channel读到Buuer中，也可以从

Buuer 写到Channel中 .

缓冲区（BuGers）

通道channel可以向缓冲区Buuer中写数据，也可以像buuer中存数据。  选择器（Selector）

使用选择器，借助单一线程，就可对数量庞大的活动 I/O 通道实时监控和维护。



|      |                                  |
| ---- | -------------------------------- |
|      | ![img](Design.assets/wps190.jpg) |

 



 

###### 3.4.4.2 特点

当一个连接创建后，不会需要对应一个线程，这个连接会被注册到多路复用器，所以一个连接只需要一个线程即  可，所有的连接需要一个线程就可以操作，该线程的多路复用器会轮训，发现连接有请求时，才开启一个线程处  理。



![img](file:///C:\Users\周壮\AppData\Local\Temp\ksohtml9152\wps191.png) 

 

如上图所示，IO模型中，一个连接来了，会创建一个线程，对应一个while死循环，死循环的目的就是不断监测这条  连接上是否有数据可以读，大多数情况下，1w个连接里面同一时刻只有少量的连接有数据可读，因此，很多个while死循环都白白浪费掉了，因为读不出啥数据。

而在NIO模型中，他把这么多while死循环变成一个死循环，这个死循环由一个线程控制，那么他又是如何做到一个  线程，一个while死循环就能监测1w个连接是否有数据可读的呢？  这就是NIO模型中selector的作用，一条连接来了之后，现在不创建一个while死循环去监听是否有数据可读了，而是直接把这条连接注册到selector上，然后，通过  检查这个selector，就可以批量监测出有数据可读的连接，进而读取数据，下面我再举个非常简单的生活中的例子   说明IO与NIO的区别

在一家幼儿园里，小朋友有上厕所的需求，小朋友都太小以至于你要问他要不要上厕所，他才会告诉你。幼儿园一  共有100个小朋友，有两种方案可以解决小朋友上厕所的问题：



![img](file:///C:\Users\周壮\AppData\Local\Temp\ksohtml9152\wps192.png)

3.4.4.3 NIO使用

简单讲完了JDK   NIO的解决方案之后，我们接下来使用NIO的方案替换掉IO的方案，我们先来看看，如果用JDK原生的NIO来实现服务端，该怎么做

NIOServer.java



|      |                                                              |
| ---- | ------------------------------------------------------------ |
|      | ![img](file:///C:\Users\周壮\AppData\Local\Temp\ksohtml9152\wps193.png) |

 





//4)绑定端口

serverSocketChannel.bind(new InetSocketAddress(port));

//5)注册,标记服务通标状态

serverSocketChannel.register(this.selector, SelectionKey.OP_ACCEPT); System.out.println("服务器启动完毕");

} catch (IOException e) { e.printStackTrace();

}

}

 

public void run(){ while (true){

try {

//1.当有至少一个通道被选中,执行此方法this.selector.select();

//2.获取选中的通道编号集合

Iterator<SelectionKey> keys = this.selector.selectedKeys().iterator();

//3.遍历keys

while (keys.hasNext()) { SelectionKey key = keys.next();

//4.当前key需要从动刀集合中移出,如果不移出,下次循环会执行对应的逻辑,造成业务错乱

keys.remove();

//5.判断通道是否有效

if (key.isValid()) { try {

//6.判断是否可读

if (key.isAcceptable()) { accept(key);

}

} catch (CancelledKeyException e) {

//出现异常断开连接key.cancel();

}

 

try {

//7.判断是否可读

if (key.isReadable()) { read(key);

}

} catch (CancelledKeyException e) {

//出现异常断开连接key.cancel();

}

 

try {

//8.判断是否可写

if (key.isWritable()) { write(key);

}

} catch (CancelledKeyException e) {

//出现异常断开连接key.cancel();

 

}



}

}

} catch (IOException e) { e.printStackTrace();

}

}

}

 

 

 

private void accept(SelectionKey key) { try {

//1.当前通道在init方法中注册到了selector中的ServerSocketChannel

ServerSocketChannel serverSocketChannel = (ServerSocketChannel) key.channel();

//2.阻塞方法, 客户端发起后请求返回.

SocketChannel channel = serverSocketChannel.accept();

///3.serverSocketChannel设置为非阻塞channel.configureBlocking(false);

//4.设置对应客户端的通道标记,设置次通道为可读时使用

channel.register(this.selector, SelectionKey.OP_READ);

} catch (IOException e) { e.printStackTrace();

}

}

 

//使用通道读取数据

private void read(SelectionKey key) { try{

//清空缓存

this.readBuffer.clear();

//获取当前通道对象

SocketChannel channel = (SocketChannel) key.channel();

//将通道的数据(客户发送的data)读到缓存中. int readLen = channel.read(readBuffer);

//如果通道中没有数据

if(readLen == -1 ){

//关闭通道key.channel().close();

//关闭连接

key.cancel(); return;

}

//Buffer中有游标,游标不会重置,需要我们调用flip重置. 否则读取不一致

this.readBuffer.flip();

//创建有效字节长度数组

byte[] bytes = new byte[readBuffer.remaining()];

//读取buffer中数据保存在字节数组readBuffer.get(bytes);

System.out.println("收到了从客户端 "+ channel.getRemoteAddress() + " : "+ new

String(bytes,"UTF-8"));

//注册通道,标记为写操作channel.register(this.selector,SelectionKey.OP_WRITE);



![img](file:///C:\Users\周壮\AppData\Local\Temp\ksohtml9152\wps194.png)

 

 

Client客户端



|      |                                                              |
| ---- | ------------------------------------------------------------ |
|      | ![img](file:///C:\Users\周壮\AppData\Local\Temp\ksohtml9152\wps195.png) |

 





channel = SocketChannel.open();

//连接远程远程服务器channel.connect(address);

Scanner sc = new Scanner(System.in); while (true){

System.out.println("客户端即将给 服务器发送数据..");

String line = sc.nextLine(); if(line.equals("exit")){

break;

}

//控制台输入数据写到缓存buffer.put(line.getBytes("UTF-8"));

//重置buffer 游标

buffer.flip();

//数据发送到数据channel.write(buffer);

//清空缓存数据

buffer.clear();

 

//读取服务器返回的数据

int readLen = channel.read(buffer); if(readLen == -1){

break;

}

//重置buffer游标buffer.flip();

byte[] bytes = new byte[buffer.remaining()];

//读取数据到字节数组buffer.get(bytes);

System.out.println("收到了服务器发送的数据 : "+ new String(bytes,"UTF-8"));

buffer.clear();

}

} catch (IOException e) { e.printStackTrace();

} finally {

if (null != channel){ try {

channel.close();

} catch (IOException e) { e.printStackTrace();

}

}

}

}

}

 

 

效果



![img](file:///C:\Users\周壮\AppData\Local\Temp\ksohtml9152\wps196.png) 



|      |                                  |
| ---- | -------------------------------- |
|      | ![img](Design.assets/wps197.jpg) |

 



 

 

 

##### 3.4.5 AIO

异步非阻塞IO。A代表asynchronize

当有流可以读时,操作系统会将可以读的流传入read方法的缓冲区,并通知应用程序,对于写操作,OS将write方法的流写  入完毕是操作系统会主动通知应用程序。因此read和write都是异步 的，完成后会调用回调函数。

使用场景：连接数目多且连接比较长（重操作）的架构，比如相册服务器。重点调用了OS参与并发操作，编程比较  复杂。Java7开始支持

 

 

 

 

### 3.5 Netty



|      |                                  |
| ---- | -------------------------------- |
|      | ![img](Design.assets/wps198.png) |

 



3.5.1 Netty认识

Netty 是由 JBOSS 提供一个异步的、 基于事件驱动的网络编程框架。

Netty 可以帮助你快速、 简单的开发出一 个网络应用， 相当于简化和流程化了 NIO 的开发过程。 作为当前最流行的 NIO 框架， Netty 在互联网领域、 大数据分布式计算领域、 游戏行业、 通信行业等获得了广泛的应用， 知名的Elasticsearch 、 Dubbo 框架内部都采用了 Netty。



![img](Design.assets/wps199.jpg) 

为什么使用Netty NIO缺点

![img](Design.assets/wps200.png)NIO 的类库和 API 繁杂，使用麻烦。你需要熟练掌握 Selector、ServerSocketChannel、SocketChannel、ByteBuuer 等.

![img](Design.assets/wps201.png)可靠性不强，开发工作量和难度都非常大

![img](Design.assets/wps202.png)NIO 的 Bug。例如 Epoll Bug，它会导致 Selector 空轮询，最终导致 CPU 100%。

Netty优点

![img](Design.assets/wps203.png)对各种传输协议提供统一的 API

![img](Design.assets/wps204.png)![img](Design.assets/wps205.png)高度可定制的线程模型——单线程、一个或多个线程池更好的吞吐量，更低的等待延迟

![img](Design.assets/wps206.png)更少的资源消耗

![img](Design.assets/wps207.png)最小化不必要的内存拷贝

 

 

#### 3.5.2 线程模型

\1. 单线程模型



|      |                                  |
| ---- | -------------------------------- |
|      | ![img](Design.assets/wps208.jpg) |

 





\2. 线程池模型



|      |                                  |
| ---- | -------------------------------- |
|      | ![img](Design.assets/wps209.jpg) |

 



 

 

\3. Netty 模型



|      |                                  |
| ---- | -------------------------------- |
|      | ![img](Design.assets/wps210.jpg) |

 



 

Netty 抽象出两组线程池， BossGroup 专门负责接收客 户端连接， WorkerGroup 专门负责网络读写操作。NioEventLoop 表示一个不断循环执行处理 任务的线程， 每个 NioEventLoop 都有一个 selector， 用于监听绑定在其上的 socket 网络通道。 NioEventLoop 内部采用串行化设计， 从消息的读取->解码->处理->编码->发送， 始终由 IO 线 程 NioEventLoop 负责。



3.5.3 Netty核心组件

ChannelHandler 及其实现类

ChannelHandler 接口定义了许多事件处理的方法， 我们可以通过重写这些方法去实现具 体的业务逻辑

我们经常需要自定义一个 Handler 类去继承 ChannelInboundHandlerAdapter， 然后通过 重写相应方法实现业务逻辑， 我们接下来看看一般都需要重写哪些方法：



|      |                                                              |
| ---- | ------------------------------------------------------------ |
|      | ![img](file:///C:\Users\周壮\AppData\Local\Temp\ksohtml9152\wps211.png) |

 



 

ChannelPipeline

ChannelPipeline 是一个 Handler 的集合， 它负责处理和拦截 inbound 或者 outbound 的事 件和操作， 相当于一个贯穿 Netty 的链。



|      |                                                              |
| ---- | ------------------------------------------------------------ |
|      | ![img](file:///C:\Users\周壮\AppData\Local\Temp\ksohtml9152\wps212.png) |

![img](file:///C:\Users\周壮\AppData\Local\Temp\ksohtml9152\wps213.png) 



 

ChannelHandlerContext

这 是 事 件 处 理 器 上 下 文 对 象 ， Pipeline 链 中 的 实 际 处 理 节 点 。 每 个 处 理 节 点ChannelHandlerContext 中 包 含 一 个 具 体 的 事 件 处 理 器 ChannelHandler ， 同 时 ChannelHandlerContext 中也绑定了对应的 pipeline 和 Channel 的信息，方便对 ChannelHandler 进行调用。 常用方法如下所示



|      |                                                              |
| ---- | ------------------------------------------------------------ |
|      | ![img](file:///C:\Users\周壮\AppData\Local\Temp\ksohtml9152\wps214.png) |

 





ChannelFuture

表示 Channel 中异步 I/O 操作的结果， 在 Netty 中所有的 I/O 操作都是异步的， I/O 的调 用会直接返回， 调用者并不能立刻获得结果， 但是可以通过 ChannelFuture 来获取 I/O 操作 的处理状态。 常用方法如下所示：



|      |                                  |
| ---- | -------------------------------- |
|      | ![img](Design.assets/wps215.png) |

 



 

EventLoopGroup 和其实现类 NioEventLoopGroup

EventLoopGroup 是一组 EventLoop 的抽象， Netty 为了更好的利用多核 CPU 资源， 一般 会有多个 EventLoop 同时工作， 每个 EventLoop 维护着一个 Selector 实例。 EventLoopGroup 提供 next 接口， 可以从组里面按照一定规则获取其中一个 EventLoop 来处理任务。 在 Netty 服务器端编程中， 我们一般都需要提供两个 EventLoopGroup， 例如： BossEventLoopGroup 和 WorkerEventLoopGroup。



|      |                                  |
| ---- | -------------------------------- |
|      | ![img](Design.assets/wps216.png) |

 



 

ServerBootstrap 和 Bootstrap

ServerBootstrap 是 Netty 中的服务器端启动助手，通过它可以完成服务器端的各种配置； Bootstrap 是 Netty 中的客户端启动助手， 通过它可以完成客户端的各种配置。 常用方法如下 所示：



|      |                                                              |
| ---- | ------------------------------------------------------------ |
|      | ![img](file:///C:\Users\周壮\AppData\Local\Temp\ksohtml9152\wps217.png) |

 



 

 

3.5.4 Netty版案例实现



|      |                                                              |
| ---- | ------------------------------------------------------------ |
|      | ![img](file:///C:\Users\周壮\AppData\Local\Temp\ksohtml9152\wps218.png) |

目标: 使用netty客户端给服务端发送数据,服务端接收消息打印. 首先引入Maven依赖



然后，下面是服务端实现部分



![img](file:///C:\Users\周壮\AppData\Local\Temp\ksohtml9152\wps219.png)NettyServer.java

 

package com.lagou.Netty;

 

import io.netty.bootstrap.ServerBootstrap; import io.netty.channel.*;

import io.netty.channel.nio.NioEventLoopGroup;

import io.netty.channel.socket.nio.NioServerSocketChannel; import io.netty.channel.socket.nio.NioSocketChannel; import io.netty.handler.codec.string.StringDecoder;

import io.netty.handler.codec.string.StringEncoder; public class NettyServer {

public static void main(String[] args) throws InterruptedException {

 

//1.创建 NioEventLoopGroup的两个实例:bossGroup workerGroup

// 当前这两个实例代表两个线程池，默认线程数为CPU核心数乘2

// bossGroup接收客户端传过来的请求

// workerGroup处理请求

EventLoopGroup bossGroup = new NioEventLoopGroup(); EventLoopGroup workerGroup = new NioEventLoopGroup();

//2、创建服务启动辅助类:组装一些必要的组件

ServerBootstrap serverBootstrap = new ServerBootstrap();

//设置组,第一个bossGroup负责连接, workerGroup负责连接之后的io处理serverBootstrap.group(bossGroup,workerGroup)

//channel方法指定服务器监听的通道类型

.channel(NioServerSocketChannel.class)

//设置channel handler , 每一个客户端连接后,给定一个监听器进行处理

.childHandler(new ChannelInitializer<NioSocketChannel>() { @Override

protected void initChannel(NioSocketChannel ch) throws Exception {

//传输通道

ChannelPipeline pipeline = ch.pipeline();

//在通道上添加对通道的处理器 , 该处理器可能还是一个监听器pipeline.addLast(new StringEncoder()); pipeline.addLast(new StringDecoder());

//监听器队列上添加我们自己的处理方式..

pipeline.addLast(new SimpleChannelInboundHandler<String>() { @Override

protected void channelRead0(ChannelHandlerContext channelHandlerContext, String s) throws Exception {

System.out.println(s);

}

});

 

}

});

 

//bind监听端口

ChannelFuture f = serverBootstrap.bind(8000).sync(); System.out.println("tcp server start success..");



![img](file:///C:\Users\周壮\AppData\Local\Temp\ksohtml9152\wps220.png)

在编写 Netty 程序时，一开始都会生成 NioEventLoopGroup 的两个实例，分别是 bossGroup 和 workerGroup，也可以称为 parentGroup 和 childGroup，为什么创建这两个实例，作用是什么？可以这么理解，bossGroup 和workerGroup 是两个线程池, 它们默认线程数为 CPU 核心数乘以 2，bossGroup 用于接收客户端传过来的请求，接收到请求后将后续操作交由 workerGroup 处理。

接下来我们生成了一个服务启动辅助类的实例 bootstrap，boostrap 用来为 Netty 程序的启动组装配置一些必须要组件，例如上面的创建的两个线程组。channel 方法用于指定服务器端监听套接字通道 NioServerSocketChannel，其内部管理了一个 Java NIO 中的ServerSocketChannel实例。

channelHandler   方法用于设置业务职责链，责任链是我们下面要编写的，责任链具体是什么，它其实就是由一个个的 ChannelHandler 串联而成，形成的链式结构。正是这一个个的 ChannelHandler 帮我们完成了要处理的事情。

ChannelInitializer 继承 ChannelInboundHandlerAdapter，用于初始化 Channel 的 ChannelPipeline。通过 initChannel

方法参数 sc 得到 ChannelPipeline 的一个实例。

当一个新的连接被接受时， 一个新的 Channel 将被创建，同时它会被自动地分配到它专属的 ChannelPipeline

通过 addLast 方法将一个一个的 ChannelHandler 添加到责任链上并给它们取个名称（不取也可以，Netty 会给它个默认名称），这样就形成了链式结构。在请求进来或者响应出去时都会经过链上这些 ChannelHandler 的处理。

最后再向链上加入我们自定义的 ChannelHandler 组件，处理自定义的业务逻辑

接着我们调用了 bootstrap 的 bind 方法将服务绑定到 8080 端口上，bind 方法内部会执行端口绑定等一系列操，使得前面的配置都各就各位各司其职，sync  方法用于阻塞当前  Thread，一直到端口绑定操作完成。最后是应用程序将会阻塞等待直到服务器的 Channel 关闭。

 

客户端NIO的实现部分

NettyClient.java



|      |                                                              |
| ---- | ------------------------------------------------------------ |
|      | ![img](file:///C:\Users\周壮\AppData\Local\Temp\ksohtml9152\wps221.png) |

 





![img](file:///C:\Users\周壮\AppData\Local\Temp\ksohtml9152\wps222.png)

使用Netty之后，一方面Netty对NIO封装得如此完美，写出来的代码非常优雅，另外一方面，使用Netty之后，网络  通信这块的性能问题几乎不用操心

 

 

3.6 

|      |                                  |
| ---- | -------------------------------- |
|      | ![img](Design.assets/wps223.png) |

基于Netty自定义RPC



RPC又称远程过程调用，我们所知的远程调用分为两种，现在在服务间通信的方式也基本以这两种为主

\1. 是基于HTTP的restful形式的广义远程调用，以spring   could的feign和restTemplate为代表，采用的协议是HTTP的7层调用协议，并且协议的参数和响应序列化基本以JSON格式和XML格式为主。

\2. 是基于TCP的狭义的RPC远程调用，以阿里的Dubbo为代表，主要通过netty来实现4层网络协议，NIO来异步传输，   序列化也可以是JSON或者hessian2以及java自带的序列化等，可以配置。

接下来我们主要以第二种的RPC远程调用来自己实现需求:

模仿  dubbo，消费者和提供者约定接口和协议，消费者远程调用提供者，提供者返回一个字符串，消费者打印提供者返回的数据。底层网络通信使用 Netty

步骤

\1. 创建一个公共的接口项目以及创建接口及方法，用于消费者和提供者之间的约定。

\2. 创建一个提供者，该类需要监听消费者的请求，并按照约定返回数据。

\3. 创建一个消费者，该类需要透明的调用自己不存在的方法，内部需要使用 Netty 请求提供者返回数据

#### 3.6.1 公共模块

首先，在公共模块中添加netty的maven依赖



|      |                                                              |
| ---- | ------------------------------------------------------------ |
|      | ![img](file:///C:\Users\周壮\AppData\Local\Temp\ksohtml9152\wps224.png) |

 



提供者及消费者工程都需依赖公共模块，这样提供者来实现接口并且提供网络调用，消费者直接通过接口来进行

TCP通信及一定的协议定制获取提供者的实现返回值

接口的定义



![img](file:///C:\Users\周壮\AppData\Local\Temp\ksohtml9152\wps225.png)

只是一个普通的接口，参数是支持序列化的String类型，返回值同理

#### 3.6.2 提供者的实现

首先是接口的实现，这一点和普通接口实现是一样的



|      |                                                              |
| ---- | ------------------------------------------------------------ |
|      | ![img](file:///C:\Users\周壮\AppData\Local\Temp\ksohtml9152\wps226.png) |

 



在实现中加入了netty的服务器启动程序，上面的代码中添加了 String类型的编解码 handler，添加了一个自定义handler

自定义 handler 逻辑如下:



![img](file:///C:\Users\周壮\AppData\Local\Temp\ksohtml9152\wps227.png)

这里显示判断了是否符合约定（并没有使用复杂的协议，只是一个字符串判断），然后创建一个具体实现类，并调  用方法写回客户端。

还需要一个启动类：



|      |                                                              |
| ---- | ------------------------------------------------------------ |
|      | ![img](file:///C:\Users\周壮\AppData\Local\Temp\ksohtml9152\wps228.png) |

 



关于提供者的代码就写完了，主要就是创建一个 netty 服务端，实现一个自定义的 handler，自定义 handler 判断是否符合之间的约定（协议），如果符合，就创建一个接口的实现类，并调用他的方法返回字符串。

#### 3.6.3 消费者相关实现

消费者有一个需要注意的地方，就是调用需要透明，也就是说，框架使用者不用关心底层的网络实现。这里我们可  以使用 JDK 的动态代理来实现这个目的。

思路：客户端调用代理方法，返回一个实现了 HelloService 接口的代理对象，调用代理对象的方法，返回结果。

我们需要在代理中做手脚，当调用代理方法的时候，我们需要初始化 Netty  客户端，还需要向服务端请求数据，并返回数据。

首先创建代理相关的类：



|      |                                                              |
| ---- | ------------------------------------------------------------ |
|      | ![img](file:///C:\Users\周壮\AppData\Local\Temp\ksohtml9152\wps229.png) |

 





![img](file:///C:\Users\周壮\AppData\Local\Temp\ksohtml9152\wps230.png)

该类有 2 个方法，创建代理和初始化客户端。

创建代理逻辑：使用 JDK 的动态代理技术，代理对象中的 invoke 方法实现如下： 如果 client 没有初始化，则初始化 client，这个 client 既是 handler ，也是一个 Callback。将参数设置进 client ，使用线程池调用 client 的 call 方法并阻塞等待数据返回

初始化客户端逻辑： 创建一个 Netty 的客户端，并连接提供者，并设置一个自定义 handler，和一些 String 类型的序列化方式。

UserClientHandler 的实现：



|      |                                                              |
| ---- | ------------------------------------------------------------ |
|      | ![img](file:///C:\Users\周壮\AppData\Local\Temp\ksohtml9152\wps231.png) |

 





![img](file:///C:\Users\周壮\AppData\Local\Temp\ksohtml9152\wps232.png)

该类缓存了 ChannelHandlerContext，用于下次使用，有两个属性：返回结果和请求参数。

当成功连接后，缓存 ChannelHandlerContext，当调用  call 方法的时候，将请求参数发送到服务端，等待。当服务端收到并返回数据后，调用 channelRead 方法，将返回值赋值个 result，并唤醒等待在 call 方法上的线程。此时，代理对象返回数据。

再看看消费者调用方式，一般的TCP的RPC只需要这样调用即可，无需关心具体的协议和通信方式：



|      |                                                              |
| ---- | ------------------------------------------------------------ |
|      | ![img](file:///C:\Users\周壮\AppData\Local\Temp\ksohtml9152\wps233.png) |

 





![img](Design.assets/wps234.png)

调用者首先创建了一个代理对象，然后每隔一秒钟调用代理的 sayHello 方法，并打印服务端返回的结果



|      |                                                              |
| ---- | ------------------------------------------------------------ |
|      | ![img](file:///C:\Users\周壮\AppData\Local\Temp\ksohtml9152\wps235.png) |

 



可以看到，消费者无需通过jar包的形式引入具体的实现项目，而是通过远程TCP通信的形式，以一定的协议和代理  通过接口直接调用了方法，实现远程service间的调用，是分布式服务的基础